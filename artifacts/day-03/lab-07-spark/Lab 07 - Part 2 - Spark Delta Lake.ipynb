{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Delta Lake features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use `spark.read.csv()` to load the data from the source public blob storage account and display its schema and shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import *\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from delta.tables import DeltaTable\r\n",
        "\r\n",
        "manualSchema = StructType([\r\n",
        "  StructField(\"CustomerId\", StringType(), True),\r\n",
        "  StructField(\"ProductId\", StringType(), True),\r\n",
        "  StructField(\"Rating\", LongType(), True),\r\n",
        "  StructField(\"Cost\", FloatType(), True),\r\n",
        "  StructField(\"Size\", FloatType(), True),\r\n",
        "  StructField(\"Price\", FloatType(), True),\r\n",
        "  StructField(\"PrimaryBrandId\", LongType(), True),\r\n",
        "  StructField(\"GenderId\", LongType(), True),\r\n",
        "  StructField(\"MaritalStatus\", LongType(), True),\r\n",
        "  StructField(\"LowerIncomeBound\", FloatType(), True),\r\n",
        "  StructField(\"UpperIncomeBound\", FloatType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\r\n",
        "raw_data = spark.read.csv(url, header=True, schema=manualSchema)\r\n",
        "print(\"Schema: \")\r\n",
        "raw_data.printSchema()\r\n",
        "\r\n",
        "df = raw_data.toPandas()\r\n",
        "print(\"Shape: \", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save the customer rating dataframe as a Delta Lake table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_table_path = 'abfss://delta@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net/customer-rating'\r\n",
        "\r\n",
        "raw_data.write.format('delta').save(delta_table_path)\r\n",
        "\r\n",
        "mssparkutils.fs.ls(delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the layout of files and inspect the Delta log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_log_path = mssparkutils.fs.ls(f'{delta_table_path}/_delta_log')[0].path\r\n",
        "print(delta_log_path)\r\n",
        "mssparkutils.fs.head(delta_log_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load the Delta lake table into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data = spark.read.format('delta').load(delta_table_path)\r\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use the dedicated `DeltaTable` class to manage the Delta Lake table. Explore the Delta Lake table history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Get all versions\r\n",
        "delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "display(delta_table.history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Perform an update on the Delta Lake table using a SQL-style condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Declare the predicate by using a SQL-formatted string.\r\n",
        "delta_table.update(\r\n",
        "  condition = \"Price < 1500\",\r\n",
        "  set = { \"Price\": \"Price * 1.05\" }\r\n",
        ")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check again the history of the Delta Lake table and notice the new entry corresponding to the update that has just been performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(delta_table.history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "It's possible to query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_table_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_table_path))\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create the metadata to expose the Delta Lake table in the default Spark database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE CustomerRating USING DELTA LOCATION '{0}'\".format(delta_table_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "List all tables that exist in the default Spark database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the properties of the `CustomerRating` Spark database table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"DESCRIBE EXTENDED customerrating\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "To query the delta table from the serverless SQL pool, navigate to the `Develop` hub in Synapse Studio and create a new SQL script. Make sure `Built-in` is selected for the `Connect to` option and `default` is selected for the `Use database` option.\r\n",
        "\r\n",
        "Enter the query below and make sure you replace `<your_data_lake_account_name>` with the name of the Data Lake account with the one from your lab environment.\r\n",
        "\r\n",
        "\r\n",
        "```sql\r\n",
        "SELECT TOP 10 *\r\n",
        "FROM OPENROWSET(\r\n",
        "    BULK 'abfss://delta@<your_data_lake_account_name>.dfs.core.windows.net/customer-rating/',\r\n",
        "    FORMAT = 'delta') as rows\r\n",
        ")\r\n",
        "```\r\n",
        "\r\n",
        "![Query Delta Lake with serverless SQL pool](https://solliancepublicdata.blob.core.windows.net/synapse-l400/notebook-images/query-delta-table.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This concludes the Delta Lake section of this notebook.\r\n",
        "\r\n",
        "To learn more about Delta Lake support in Syanspe Spark, take a look at the [Work with Delta Lake](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python) section in the Azure Synapse Analytics documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Mount storage account containers\r\n",
        "\r\n",
        "The `mssparkutils` utility can be used to mount storage account containers. In the example below, you will use an already created linked service to manage the authentication with the storage account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool02",
              "session_id": 7,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:44:59.0495819Z",
              "session_start_time": null,
              "execution_start_time": "2022-02-08T12:44:59.1814619Z",
              "execution_finish_time": "2022-02-08T12:45:22.0296104Z"
            },
            "text/plain": "StatementMeta(SparkPool02, 7, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "mssparkutils.fs.mount( \r\n",
        "    \"abfss://delta@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net\", \r\n",
        "    \"/test\", \r\n",
        "    {\"linkedService\":\"#DATA_LAKE_ACCOUNT_NAME#\"} \r\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the content of the mounted volume using the local path. Note the `synfs:/{jobId}` prefix used by `mssparkutils` for the local path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool02",
              "session_id": 7,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:45:31.274196Z",
              "session_start_time": null,
              "execution_start_time": "2022-02-08T12:45:31.3644754Z",
              "execution_finish_time": "2022-02-08T12:45:31.5152351Z"
            },
            "text/plain": "StatementMeta(SparkPool02, 7, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "[FileInfo(path=synfs:/7/test/customer-rating/_delta_log/00000000000000000000.json, name=00000000000000000000.json, size=2299),\n FileInfo(path=synfs:/7/test/customer-rating/_delta_log/00000000000000000001.json, name=00000000000000000001.json, size=1466)]"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "jobId = mssparkutils.env.getJobId() \r\n",
        "\r\n",
        "log_files = mssparkutils.fs.ls(f'synfs:/{jobId}/test/customer-rating/_delta_log')\r\n",
        "log_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "When using regular PySpark classes, the syntax of the prefix is slightly different - `/synfs/{jobId}`. Use this prefix to load and display the first 500 characters from the first Delta Lake log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool02",
              "session_id": 7,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:45:33.3618394Z",
              "session_start_time": null,
              "execution_start_time": "2022-02-08T12:45:33.4552981Z",
              "execution_finish_time": "2022-02-08T12:45:33.9521758Z"
            },
            "text/plain": "StatementMeta(SparkPool02, 7, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "'{\"commitInfo\":{\"timestamp\":1644308193124,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"ErrorIfExists\",\"partitionBy\":\"[]\"},\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"1\",\"numOutputBytes\":\"135572\",\"numOutputRows\":\"5000\"}}}\\n{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\\n{\"metaData\":{\"id\":\"ae14ef8d-1182-44af-9a85-76f991697c24\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\\\"type\\\\\":\\\\\"struct\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"CustomerId\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\",\\\\\"nullable\\\\\"'"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with open(f'/synfs/{jobId}/test/customer-rating/_delta_log/{log_files[0].name}', 'r') as f:\r\n",
        "    f.read()[:500]"
      ]
    }
  ]
}
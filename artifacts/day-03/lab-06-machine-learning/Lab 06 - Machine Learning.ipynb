{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a classifier to determine product seasonality\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See installed packages\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pkg_resources\n",
        "for d in pkg_resources.working_set:\n",
        "     print(d)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all necessary libraries.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from onnxmltools.convert import convert_xgboost\n",
        "from onnxmltools.convert.common.data_types import FloatTensorType\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory data analysis (basic stats)\n",
        "\n",
        "Create Spark temporary views for sales and products.\n",
        "\n",
        "**IMPORTANT!** Make sure the name of the SQL pool (`SQLPool01` below) matches the name of your SQL pool.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Seasonality"
            ],
            "values": [
              "ProductId"
            ],
            "yLabel": "ProductId",
            "xLabel": "Seasonality",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"ProductId\":{\"1\":771373,\"2\":247930,\"3\":262978}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "%%spark\n",
        "val df = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.SaleSmall\") \n",
        "df.createOrReplaceTempView(\"sale\")\n",
        "\n",
        "val df2 = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.Product\") \n",
        "df2.createOrReplaceTempView(\"product\")\n",
        "display(df2)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load daily product sales from the SQL pool.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "sqlQuery = \"\"\"\n",
        "SELECT\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "    ,COUNT(*) as TransactionItemsCount\n",
        "FROM\n",
        "    sale S\n",
        "    JOIN product P ON\n",
        "        S.ProductId = P.ProductId\n",
        "WHERE\n",
        "    S.TransactionDateId NOT IN (20120229, 20160229)\n",
        "GROUP BY\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "\"\"\"\n",
        "\n",
        "prod_df = spark.sql(sqlQuery)\n",
        "prod_df.cache()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the number of records in the data farame (should be around 13 million rows)."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_df.count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display some statistics about the data frame.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "summary"
            ],
            "values": [
              "summary"
            ],
            "yLabel": "summary",
            "xLabel": "summary",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{\"summary\":{\"count\":1,\"max\":1,\"mean\":1,\"min\":1,\"stddev\":1}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(prod_df.describe())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pivot the data frame to make daily sale items counts columns. \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_df.groupBy(['ProductId', 'Seasonality']).pivot('TransactionDateId').sum('TransactionItemsCount').toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean up the nulls and take a look at the result.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_prep_df.fillna(0)\n",
        "prod_prep_df.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Isloate features and prediction classes.\n",
        "\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X = prod_prep_df.iloc[:, 2:].values\n",
        "y = prod_prep_df['Seasonality'].values\n",
        "\n",
        "X_scale = StandardScaler().fit_transform(X)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use PCA for dimensionality reduction\n",
        "\n",
        "Perform dimensionality reduction using Principal Components Analysis and two target components.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "pca_df = pd.DataFrame(data = principal_components, columns = ['pc1', 'pc2'])\n",
        "pca_df = pd.concat([pca_df, prod_prep_df[['Seasonality']]], axis = 1)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the products data frame in two dimensions (mapped to the two principal components).\n",
        "\n",
        "Note the clear separation of clusters.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize = (6,6))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [1, 2, 3]\n",
        "colors = ['r', 'g', 'b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = pca_df['Seasonality'] == target\n",
        "    ax.scatter(pca_df.loc[indicesToKeep, 'pc1']\n",
        "               , pca_df.loc[indicesToKeep, 'pc2']\n",
        "               , c = color\n",
        "               , s = 1)\n",
        "ax.legend(['All Season Products', 'Summer Products', 'Winter Products'])\n",
        "ax.plot([-0.05, 1.05], [0.77, 1.0], linestyle=':', linewidth=1, color='y')\n",
        "ax.plot([-0.05, 1.05], [0.37, 0.6], linestyle=':', linewidth=1, color='y')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo the Principal Components Analysis, this time with twenty dimensions.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def col_name(x):\n",
        "    return f'f{x:02}'\n",
        "\n",
        "pca = PCA(n_components=20)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "X = pd.DataFrame(data = principal_components, columns = list(map(col_name, np.arange(0, 20))))\n",
        "pca_df = pd.concat([X, prod_prep_df[['ProductId']]], axis = 1)\n",
        "pca_automl_df = pd.concat([X, prod_prep_df[['Seasonality']]], axis = 1)\n",
        "\n",
        "X = X[:4500]\n",
        "y = prod_prep_df['Seasonality'][:4500]\n",
        "pca_automl_df = pca_automl_df[:4500]"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the PCA components to the SQL pool. - the `spark.sql.execution.arrow.fallback.enabled` warning can be safely ignored.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca_sdf = spark.createDataFrame(pca_df)\n",
        "pca_sdf.createOrReplaceTempView(\"productpca\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%spark\n",
        "// Make sure the name of the SQL pool (#SQL_POOL_NAME# below) matches the name of your SQL pool.\n",
        "val df = spark.sqlContext.sql(\"select * from productpca\")\n",
        "df.write.sqlanalytics(\"#SQL_POOL_NAME#.wwi_ml.ProductPCA\", Constants.INTERNAL)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train ensemble of trees classifier (using XGBoost)\n",
        "\n",
        "Split into test and training data sets.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the ensemble classifier using XGBoost.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform predictions with the newly trained model.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the accuracy of the model using test data.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert trained model to ONNX format.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "initial_types = [\n",
        "    ('input', FloatTensorType([1, 20]))\n",
        "]\n",
        "\n",
        "onnx_model = convert_xgboost(model, initial_types=initial_types)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train classifier using Auto ML\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.automl.runtime.onnx_convert import OnnxConverter"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca_automl_df.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the connection to the Azure Machine Learning workspace. The Azure portal provides all the values below.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "subscription_id='#SUBSCRIPTION_ID#'         # ensure it matches your Azure subscription id\n",
        "resource_group='#RESOURCE_GROUP_NAME#'      # ensure it matches your resource group name\n",
        "workspace_name='#AML_WORKSPACE_NAME#'       # ensure it matches your Azure Machine Learning workspace name\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "ws.write_config()\n",
        "ws = Workspace.from_config()\n",
        "experiment = Experiment(ws, \"Product_Seasonality\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the Automated Machine Learning experiment and start it (will run on local compute resources).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "automl_classifier_config = AutoMLConfig(\n",
        "        task='classification',\n",
        "        #experiment_exit_score = 0.995,\n",
        "        experiment_timeout_minutes=15,\n",
        "        enable_onnx_compatible_models=True,\n",
        "        training_data=pca_automl_df,\n",
        "        label_column_name='Seasonality',\n",
        "        n_cross_validations=5,\n",
        "        enable_voting_ensemble=False,\n",
        "        enable_stack_ensemble=False\n",
        "        )\n",
        "\n",
        "local_run = experiment.submit(automl_classifier_config, show_output=True)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve the best model directly in ONNX format and take a look at it.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "best_run, onnx_model2 = local_run.get_output(return_onnx_model=True)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "onnx_model2"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace below the placeholders with the name of the primary data lake account and one of it's security keys."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "modelPath = \"abfss://wwi-02@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net/ml/onnx/product_seasonality_classifier.onnx\"\n",
        "modelString = str(onnx_model2.SerializeToString())\n",
        "mssparkutils.fs.put(modelPath, modelString)"
      ],
      "attachments": {}
    }
  ]
}